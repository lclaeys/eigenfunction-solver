{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/lclaeys/eigenfunction-solver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.energy.quadratic import QuadraticEnergy\n",
    "from src.eigensolver.neural.network.feedforward import FeedForwardNetwork, ConstantFFN\n",
    "from src.eigensolver.neural.loss.orth_loss import BasicOrthogonalityLoss, CovOrthogonalityLoss\n",
    "from src.eigensolver.neural.loss.variational_loss import VariationalLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "m = 3\n",
    "N = 50000\n",
    "energy = QuadraticEnergy(np.eye(dim))\n",
    "x = torch.tensor(energy.exact_sample((N,)), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConstantFFN([dim,200,200,2000,200,m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_loss = VariationalLoss()\n",
    "orth_loss = BasicOrthogonalityLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "#optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)  # SGD with momentum\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_outputs = torch.eye(m)[:,None,:].expand([m,N,m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    x,\n",
    "    batch_size=5000,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 0/10, lr: 8.95e-03, Loss: 1.999e+00, Loss_1: 8.577e-04, Loss_2: 1.999e+00\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m var_cost\u001b[38;5;241m.\u001b[39mappend(loss_1)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m*\u001b[39mloss_1 \u001b[38;5;241m+\u001b[39m loss_2\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[1;32m     22\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beta = 0.5\n",
    "orth_cost = []\n",
    "var_cost = []\n",
    "for epoch in range(10000):  # Train for 100 epochs\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = data.requires_grad_()\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        fx = model(data)  # Forward pass\n",
    "\n",
    "        grad_outputs = torch.eye(m)[:,None,:].expand([m,data.shape[0],m])\n",
    "\n",
    "        grad_fx = torch.autograd.grad(outputs = fx, inputs = data, grad_outputs = grad_outputs, is_grads_batched=True, create_graph=True)[0].transpose(0,1)\n",
    "        loss_1 = var_loss(grad_fx)  # Variational loss\n",
    "        loss_2 = orth_loss(fx)\n",
    "        \n",
    "        orth_cost.append(loss_2)\n",
    "        var_cost.append(loss_1)\n",
    "\n",
    "        loss = beta*loss_1 + loss_2\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update model parameters\n",
    "        scheduler.step()\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}, batch {batch_idx}/{len(dataloader)}, lr: {scheduler.get_last_lr()[0]:.2e}, Loss: {loss.item():.3e}, Loss_1: {loss_1.item():.3e}, Loss_2: {loss_2.item():.3e}\", end = '\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [-0.0339, -0.0256],\n",
       "         [-0.0476, -0.0334]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [-0.0162, -0.0449],\n",
       "         [-0.0469, -0.0212]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [-0.0200, -0.0437],\n",
       "         [-0.0492, -0.0272]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [ 0.0126, -0.0050],\n",
       "         [-0.0093, -0.0101]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [-0.0025, -0.0072],\n",
       "         [-0.0365,  0.0033]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [ 0.0123, -0.0022],\n",
       "         [-0.0229, -0.0022]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    torch.nn.init.uniform_(param, a=-1.0, b=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5148,  0.3357],\n",
      "        [-1.2070,  0.2114],\n",
      "        [ 0.7853, -0.7413],\n",
      "        ...,\n",
      "        [-1.0368, -1.3482],\n",
      "        [-0.6729,  1.2756],\n",
      "        [-0.3227,  0.2393]], requires_grad=True)\n",
      "tensor([[-0.3194, -0.5795,  0.0000],\n",
      "        [ 0.5039, -0.3986,  0.0000],\n",
      "        [-0.7475,  0.7599,  0.0000],\n",
      "        ...,\n",
      "        [ 0.6164,  0.4137,  0.0000],\n",
      "        [ 0.7695, -0.4592,  0.0000],\n",
      "        [ 0.5631, -0.4437,  0.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = data.requires_grad_()\n",
    "# fx = 1/6*(data)**3 @ torch.tensor([[1,0,0],[0,1,0]],dtype=torch.float64)\n",
    "# fx = model.forward(data)\n",
    "\n",
    "grad_outputs = torch.eye(m)[:,None,:].expand([m,data.shape[0],m])\n",
    "grad_fx = torch.autograd.grad(outputs = fx, inputs = data, grad_outputs = grad_outputs, is_grads_batched=True, create_graph=True)[0].transpose(0,1)\n",
    "\n",
    "laplacian = torch.zeros(data.shape[0], m)  # Shape: [batch_size, m]\n",
    "for i in range(data.shape[1]):  # Loop over input dimensions\n",
    "    second_derivatives = torch.autograd.grad(\n",
    "        outputs=grad_fx[:, :, i],  # First derivative w.r.t. data[:, i]\n",
    "        inputs=data,\n",
    "        grad_outputs=grad_outputs,  # Scalar tensor for gradients\n",
    "        is_grads_batched=True,\n",
    "        create_graph=True\n",
    "    )[0].transpose(0,1)[:, :, i]  # Extract diagonal second derivative for input i\n",
    "    laplacian += second_derivatives\n",
    "print(data)\n",
    "print(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vmap: inplace arithmetic(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over in a vmap. Please try to use out-of-place operators instead of inplace arithmetic. If said operator is being called inside the PyTorch framework, please file a bug report instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _laplacian_wrt_x\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#We define a laplacian func for a single function, then vectorize over the batch.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m laplacian \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_laplacian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m, in \u001b[0;36mcompute_laplacian\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_laplacian\u001b[39m(params, x):\n\u001b[0;32m---> 10\u001b[0m   _hessian_wrt_x \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#forward-over-reverse hessian calc.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   _laplacian_wrt_x \u001b[38;5;241m=\u001b[39m _hessian_wrt_x\u001b[38;5;241m.\u001b[39mdiagonal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#use relative dims for vmap (function doesn't see the batch dim of the input)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _laplacian_wrt_x\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:1310\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     _, jvp_out \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[0;32m-> 1310\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1312\u001b[0m     results, aux \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:1299\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_jvp\u001b[39m(basis):\n\u001b[0;32m-> 1299\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;241m0\u001b[39m], is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:1139\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m   1138\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1139\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m, in \u001b[0;36mfcall\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfcall\u001b[39m(params, x):\n\u001b[0;32m----> 7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_functorch/functional_call.py:148\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/utils/stateless.py:298\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    294\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    296\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    297\u001b[0m ):\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/eigenfunction-solver/src/eigensolver/neural/network/feedforward.py:93\u001b[0m, in \u001b[0;36mConstantFFN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     92\u001b[0m output[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Assign constant value\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)  \u001b[38;5;66;03m# Fill the remaining dimensions\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vmap: inplace arithmetic(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over in a vmap. Please try to use out-of-place operators instead of inplace arithmetic. If said operator is being called inside the PyTorch framework, please file a bug report instead."
     ]
    }
   ],
   "source": [
    "from torch.func import hessian, vmap, functional_call\n",
    "\n",
    "params = dict(model.named_parameters()) #params (need to be an input for torch.func to work)\n",
    "\n",
    "#functionalize version (params are now an input to the model)\n",
    "def fcall(params, x):\n",
    "  return functional_call(model, params, x)\n",
    "\n",
    "def compute_laplacian(params, x):\n",
    "  _hessian_wrt_x = hessian(fcall, argnums=1)(params, x) #forward-over-reverse hessian calc.\n",
    "  _laplacian_wrt_x = _hessian_wrt_x.diagonal(0,-2,-1) #use relative dims for vmap (function doesn't see the batch dim of the input)\n",
    "  return _laplacian_wrt_x\n",
    "\n",
    "#We define a laplacian func for a single function, then vectorize over the batch.\n",
    "laplacian = vmap(compute_laplacian, in_dims=(None,0))(params, x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_derivatives = torch.autograd.grad(\n",
    "        outputs=grad_fx[:, :, 0],  # First derivative w.r.t. data[:, i]\n",
    "        inputs=data,\n",
    "        grad_outputs=grad_outputs,  # Scalar tensor for gradients\n",
    "        is_grads_batched=True,\n",
    "        create_graph=True,\n",
    "        #allow_unused=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[0][None,:])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda x: model(x[None,:])[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3739.4986, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fn(x[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.hessian(model_fn, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_fx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/__init__.py:469\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    468\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(outputs))\n\u001b[0;32m--> 469\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/__init__.py:198\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outputs = grad_fx, inputs = data, grad_outputs = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grad_fx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/__init__.py:492\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _engine_run_backward(\n\u001b[1;32m    483\u001b[0m             outputs,\n\u001b[1;32m    484\u001b[0m             gO,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m             accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    490\u001b[0m         )\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap_internals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none_pass_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_vmap_internals.py:231\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    229\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    230\u001b[0m     )\n\u001b[0;32m--> 231\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    233\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/__init__.py:482\u001b[0m, in \u001b[0;36mgrad.<locals>.vjp\u001b[0;34m(gO)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(gO):\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "grad_fx = torch.autograd.grad(outputs = fx, inputs = data, grad_outputs = grad_outputs, is_grads_batched=True, create_graph=True)[0].transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10.0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGiCAYAAABH4aTnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALN5JREFUeJzt3Xl8VOWh//HvTCYzWWeyb5BA2GUV2YzWraZSiwt2udZL+7PY1taL28XbW7n3pdZrbWztz2ut/mhvF6G3KrW2qFVRKQqIsu+LQoAACZAFSGayTiYzz++PyNQIisjkSSKf9+s1rzpnTuY88yQ6n54554zDGGMEAABgibOnBwAAAM4uxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALDqtONj+fLluvrqq1VQUCCHw6Hnn3++y+PGGN17773Kz89XYmKiSktLVV5eHqvxAgCAPu6046O5uVnjxo3TE088cdLHf/azn+mxxx7Tr371K61evVrJycmaOnWq2trazniwAACg73OcyRfLORwOLVy4UNOnT5fUudejoKBAd911l/7t3/5NkuT3+5Wbm6t58+bp61//ekwGDQAA+i5XLJ+soqJC1dXVKi0tjS7z+XyaMmWKVq5cedL4CAaDCgaD0fuRSETHjh1TZmamHA5HLIcHAAC6iTFGjY2NKigokNP58R+sxDQ+qqurJUm5ubldlufm5kYf+7CysjLdf//9sRwGAADoIZWVlerfv//HrhPT+Pg05syZo9mzZ0fv+/1+FRUVqbKyUl6vtwdHBgAAPqlAIKDCwkKlpqaect2YxkdeXp4kqaamRvn5+dHlNTU1Ovfcc0/6Mx6PRx6P54TlXq+X+AAAoI/5JIdMxPQ6H8XFxcrLy9OSJUuiywKBgFavXq2SkpJYbgoAAPRRp73no6mpSbt3747er6io0KZNm5SRkaGioiLdeeed+vGPf6yhQ4equLhY99xzjwoKCqJnxAAAgLPbacfHunXrdNlll0XvHz9e48Ybb9S8efP07//+72pubtbNN9+shoYGfe5zn9Orr76qhISE2I0aAAD0WWd0nY/uEAgE5PP55Pf7OeYDAIA+4nTev/luFwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFgV8/gIh8O65557VFxcrMTERA0ePFgPPPCAjDGx3hQAAOiDXLF+wp/+9KeaO3eu5s+fr1GjRmndunWaOXOmfD6fbr/99lhvDgAA9DExj4933nlH1157raZNmyZJGjhwoJ555hmtWbMm1psCAAB9UMw/drngggu0ZMkS7dq1S5K0efNmrVixQldeeeVJ1w8GgwoEAl1uAADgsyvmez7uvvtuBQIBjRgxQnFxcQqHw3rwwQc1Y8aMk65fVlam+++/P9bDAAAAvVTM93w8++yzeuqpp/T0009rw4YNmj9/vn7+859r/vz5J11/zpw58vv90VtlZWWshwQAAHoRh4nxaSiFhYW6++67NWvWrOiyH//4x/rjH/+o995775Q/HwgE5PP55Pf75fV6Yzk0AADQTU7n/Tvmez5aWlrkdHZ92ri4OEUikVhvCgAA9EExP+bj6quv1oMPPqiioiKNGjVKGzdu1COPPKKbbrop1psCAAB9UMw/dmlsbNQ999yjhQsXqra2VgUFBbrhhht07733yu12n/Ln+dgFAIC+53Tev2MeH2eK+AAAoO/p0WM+AAAAPg7xAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFjVLfFx8OBBfeMb31BmZqYSExM1ZswYrVu3rjs2BQAA+hhXrJ+wvr5eF154oS677DItWrRI2dnZKi8vV3p6eqw3BQAA+qCYx8dPf/pTFRYW6sknn4wuKy4ujvVmAABAHxXzj11efPFFTZw4UV/72teUk5Oj8ePH6ze/+c1Hrh8MBhUIBLrcAADAZ1fM42Pv3r2aO3euhg4dqtdee0233HKLbr/9ds2fP/+k65eVlcnn80VvhYWFsR4SAADoRRzGGBPLJ3S73Zo4caLeeeed6LLbb79da9eu1cqVK09YPxgMKhgMRu8HAgEVFhbK7/fL6/XGcmgAAKCbBAIB+Xy+T/T+HfM9H/n5+Ro5cmSXZeecc44OHDhw0vU9Ho+8Xm+XGwAA+OyKeXxceOGF2rlzZ5dlu3bt0oABA2K9KQAA0AfFPD7+9V//VatWrdJPfvIT7d69W08//bT+53/+R7NmzYr1pgAAQB8U8/iYNGmSFi5cqGeeeUajR4/WAw88oEcffVQzZsyI9aYAAEAfFPMDTs/U6RywAgAAeocePeAUAADg4xAfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXEBwAAsIr4AAAAVhEfAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFXdHh8PPfSQHA6H7rzzzu7eFAAA6AO6NT7Wrl2rX//61xo7dmx3bgYAAPQh3RYfTU1NmjFjhn7zm98oPT39I9cLBoMKBAJdbgAA4LOr2+Jj1qxZmjZtmkpLSz92vbKyMvl8vuitsLCwu4YEAAB6gW6JjwULFmjDhg0qKys75bpz5syR3++P3iorK7tjSAAAoJdwxfoJKysrdccdd2jx4sVKSEg45foej0cejyfWwwAAAL2UwxhjYvmEzz//vK677jrFxcVFl4XDYTkcDjmdTgWDwS6PfVggEJDP55Pf75fX643l0AAAQDc5nffvmO/5uPzyy7V169Yuy2bOnKkRI0bohz/84ceGBwAA+OyLeXykpqZq9OjRXZYlJycrMzPzhOUAAODswxVOAQCAVTHf83EyS5cutbEZAADQB7DnAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFbFPD7Kyso0adIkpaamKicnR9OnT9fOnTtjvRkAANBHxTw+li1bplmzZmnVqlVavHixQqGQrrjiCjU3N8d6UwAAoA9yGGNMd26grq5OOTk5WrZsmS6++OITHg8GgwoGg9H7gUBAhYWF8vv98nq9MRtHbWOb7nthu1xxTrmcDsU5HYqP6/xfl7NzmSvOqYR4p9IS45WW5JYvKV6+xHjl+xKU502Qw+GI2XgAAPgsCQQC8vl8n+j929Xdg/H7/ZKkjIyMkz5eVlam+++/v7uHoUBrhxZtq/7UP5/icWloborOLUzTJcOydcmwbGIEAIBPoVv3fEQiEV1zzTVqaGjQihUrTrqOrT0fDS3t+tvmQ+qIGHWEzfv/G1FHxCgcMQpFIgqHjZrbwwq0htTQ2q6GlpAaWkKqDrQpHOk6TZMGpus/vnSOxhelx2yMAAD0Vaez56Nb4+OWW27RokWLtGLFCvXv3/8T/czpDN6W9o6I9h9t1o7DAa2uOKa/bqhSWygiSfr6pEL96JpRSoiP6+FRAgDQc3pFfNx666164YUXtHz5chUXF3/in+uN8fFhh/2t+r+v79Jz66skSdPG5Ovxfx7PxzAAgLPW6bx/x/xsF2OMbr31Vi1cuFBvvPHGaYVHX5HvS9TPvzZO82ZOUnycQy9vPawXNx/q6WEBANAnxDw+Zs2apT/+8Y96+umnlZqaqurqalVXV6u1tTXWm+pxlw7P0e2fHypJ+tmrO9UWCvfwiAAA6P1iHh9z586V3+/XpZdeqvz8/OjtT3/6U6w31St856JByvcl6GBDq363oqKnhwMAQK/XLR+7nOz2rW99K9ab6hUS3XH6wdThkqTfr6hQRzjSwyMCAKB347tdYuDqcQVKT4rX0eZ2ra441tPDAQCgVyM+YiA+zqkvjs6XJL20hQNPAQD4OMRHjFw1tjM+Fm2rVoiPXgAA+EjER4xMKc5QVopbDS0hvb37SE8PBwCAXov4iBFXnFNTR+VJkt58r7aHRwMAQO9FfMTQRUOzJEnv7DnawyMBAKD3Ij5iaEpxphwOqby2SXWNwVP/AAAAZyHiI4bSk906J6/zevbLdtVFl7e2c+VTAACOIz5i7IpRuZKkV7YeVlV9i74y9x2dc++rmv7E22oKdqjyWIv+vK5Su2ubenikAAD0jG77VttPqy98q+3HKa9p1Bf+e7kkye1yqr3jH6fd3lgyQM+sqVR7OCKHQ/rpl8fqnyYV9tRQAQCImR79Vtuz3dDcVF0+IkeS1N4RUWFGoq4Y2bk3ZP7K/Wp//xogxkh3/3WL/r6jRsYYPbPmgB58eYfaQmHtrWvSj17crqdW7+dy7QCAzxxXTw/gs+hX35ygv26oUmNbh64b30+7apr0+o6a6ON/n32JHltSrhc3H9J3/rBOifFxan3/G3H9rSFtPNCg8vc/llm/v14PTh+jt3cf0cJNBzVjcpEmF2eoKdihN96rVWpCvAozElWclaz9R1s0KCtZrjinjDFqaQ/rnT1HdeGQTCW5T/6rNsaoJhBUrtej4/vAnE5Hl8cdjs77oXBEC9YcUL/0RH1+RG70cWO6/owkVRxpVp43QYnuuFPOVygckb81pKwUzyecYQBAX8bHLhaEwhF96Rdvqby2SV8Ymavf/J+JCrSFNOupDXpnz1GFIx//K0iId6otFPnI+ydbPxQ20ef1uJzyJsYryR0nd1znzq76lpAkoyNN7ZKkfF+CAq0hZad6dPGwbDW0hLT3SJN2VjdqTD+fPjc0W0verdH2Q4Ho+qMKvNpxKKCaxqCG56aqub1D/dMT5Y5z6s2ddcrzJmjqqFz5W0NqCoY1JCdFw3JTNCQnRQ0tIf11Q5UK0hL1ytbD2ne0RRnJbg3JSdElw7I1ODtFr22vljvOqTH9fSrKSFKuN0HhiFGO16NIxGh3bZMyUzzqn56od/YcVYrHpbSkeJ2T71VLe4cS4+NkjLSq4qhyUhM0JCflI+csHDFyOhQNLQDA6Tmd92/iw5KOcET1LSFlpbi7vME1toW0qbJBw3NT9eLmQ3rwlXeVkeTW4/98ngJtIT3w0g5V1bdG109yx6nl/bNnEuKdGpydon1HmtXMGTVROake1TYGleyOUyhsoh91XTo8WzWBoAKtIU0uzlCKx6VEd5yONAb16vZqDcpO1q2XDdWhhlaFwhF9+bz+CnaENf+dfUqIj9PnhmSpIC1RhRlJPfwKAaD3IT76sMpjLUpPdivF0/kxSThitKmyXqkJ8RqWm6pIxKjiaLOq6lt1XlGaUhPiFewIa8m7teqXlqjGtg5lprjVFOzQsp11+vw5OXKo8+DX5mBY7R0RtbR3KNeboIMNrfr7jhp9YWSu0pLcCrSFtKL8iBwOKTE+Tk6nQ5MHZmjRtsMyRhqYlayLhmbpJ6+8q8L0JA3NTVGgtUObqxo0ODtFXxiZq02VDTra1K7LRmTrwLEWba3yqzUU1vC8VB1qaFV5TZPKa5vkdEiDslMU7IgoxROnndWNyk5N0JdG5+ntPUdUVd+qi4dlyx3nVFV9i3bWNKquMagkt0vHmjv31vRLS9TR5mB0L5DTIZ1iJ9Jp+fABw8f5EuPlcXXuQRqel6pBWclqbg9rWG6KvjqhUOlJ8WoMdijV45LD4ZAxRrtqmjQwK0ke16k/hgKAvoj4QJ/zwWNLTiUUjqg1FJY3IV7NwQ49t75Kk4szdE6+V5XHWrT3SLMGZyerpT2sxPjON/v7/7ZdEdO5ndH9fEp0x+nA0RYt21WnlvawXE6HhuSkqK4xqOKsZB1pCmpzlV+SlOpxSQ6psa3jtF7ToKxkZaV61NDSrl01TfK4nCoZnKmijCSNKvDq8nNylZns5qMeAJ8JxAdwhowx2nCgXof9bbp4WLacDodagh3acTjw/jEsHSpMT1JVfav2HW1Wgsupl7Ye1t665tPazqCsZBVnJaswI0mpCS69tOWwBmQm6euTCjU4u/P4mEBbh3yJ8d30SgEgNogPoAccP3Po5a2H1T89Uf6WkF7YfFDGSLMuG6J9R5tVXtOk3bVN2nrQL39r6GOfz+mQPK44BTvCun5SkZqDHSrOStY3SwZwZhCAXof4AHo5Y4yagh1aurNOmysb9Fb5EQXaQpo0MENvlde9fzbSRxuem6pxhT5dNjxHmSkeLVhzQHdfOUIt7WE98eZuBTsimjIoQ1+bUCi3q3ddzmf9/noNyEzqsYAqr2lUUWbvOv7GGKNQ2ER/Vy3tHR95enys1Te3a3l5naaNyZcrLnZ/K8YYVR5rVWFGoiTpte01GpabokHZH33W2ceNcWNlvS4bnhPTjykjEaP6lnZlnuHfYkNLu3yJ8b36I9RQOKK9dc0anpeqLVUNGl3gO+ESCWeK+AD6sLrGoPytIR1rbtdb5XWqONKsTZUNXc56+qTOK0rTf04bqb9tPqR+aYl6eeth1TUG9d2LiuV2xWnjgXr5W0NyODr3zoztn6a/76hRoC2kq8cVSJLi45yqb26XvzWk/LQEPb/xoMprmvTNkgFyyKGCtAT9bkWFdtc26dbPD9Fz66v01Qn9tXrvMQU7wvpmyUBJUrAjrMff2K1fvrFb6Unx+vU3J2pycYbaQmGt3HNUY/v7tLy8To1tHfqniYVKiP9HHOyta9Iji3fpkmHZSva49PkROV0el6RdNY365u9W6+uTijQoO1nGSFeN7XxDDbSFNPtPm7V0Z606IiZ6yrvUecbZC5sOqWRwpgZnp8gYoy1Vfg3PS1VCfJy2H/Lrkdd36dIROfrm+QNO+3cQCkcUH+fUqr1HVdcYVOk5udHr3zQHO/TEm7u1fn+9NlU26KnvTFGgLaSb5q3T9y4epDlfOkeVx1r0g+c26+uTijR9fD8t3lGjhHinLhqa/bHb9beG9PTqA5oyKEMel1PBjoiykj1aXl6nFzYd1JTiTM3+wjD98C9b9Of1VfrWBQP1o2tGacm7NXpk8S7dc9VInT8oU4t31OiuZzfpriuG65vnDzjhDcsYo3cPN6pfeqI8Lqci74fUgjUHVLboPd1z1UileOL0w79sVZ43QUvuukTJno8Oq+Pz9cHnv/7Xq7Rm3zFJ0hdG5uqRfxqn3bVNnXsCf7dGw/NS9bOvjNX3/rhe7x4OaOG/XKi6xqD+sHKf7iwdpjxfQvT5OsIR3fbMRiV7XCrOStbDr+3Uj64eqW9dWBxdZ2uVX+9VBzRtbP4JEVgbaNNv3tqrmRcWqyAtUQs3Vmn2s5t1w+Qi/eS6MSe8noUbq+R0OHTV2AIdrO+MseOR0hbqPFnA7XJqWG6KvAnx2lPXpOW76jTj/M49nHEfmO9qf5vSkjoPeK9rCqqtPaKizCSt2ntUf99Rozu/MEwpHpfe2X1Ev1hSrkuH52hEfqouGZqtG59co7fKj+i+q0fqv17aoUFZyfrbbZ+LaeQSH8BnVCRitGrvUS1+t0ZtoYgWrD2gD/8bPK4wTRcNydL8lftO+yBZd5wzemqyJMXHOXRjyUD9eX3VR35M5E1wKfAx27n23AJNGpihXy3b0yWgnA6pf3qSjjYFTzhVfGBmkqaP76dtBwP6+7s1H35KFfgSNOP8AUqMj9P6/fVqC4UVihgt/8AXOh7nS4yXMeaEMc65coTSk9z6/dsVeq+6UXFOh6aOylVNIKj1++slSVeOztPf361RKNw5yVeNzVdRRpIGZiVrd22Tth/y68CxFvkS45WW6FZVfYtaQ2Fdfk6uNh5o0LuHO6+Lc/zNX5Iykt26ZlyB6lvatWhbdZczqvJ9CRqQmaRVezvfaCcMSI+ORZJ+9pWx+ve/bJHUeer4wMxkORydgTj93H5auLHz99QRMfrrhoMf+Ts57lffmKDv/3F99P5z3y/RV3+1Mnr/wiGZenv30eh9t8up0nNylJOaIG+CS9WBNpXXNmnjgYYuz/vhv6PUBFf0b/G2zw/RsNxUvbDpoOqa2nXx0CxdNbZAmysbtGrvUb289bCuG99PX53QX0lulyrrW/S9/12vU/nexYP06+V7JUnZqZ7oN4sX+BL0h29PVkNLSG/vPqr5K/dFz5g7Lj7OoUevHy9vokvz39kf/ZvLTHarZHCmslI8unpcvo40tWv2nzapuT2sCwZn6mdfHavLfr40+vcxrjBND391rHYcCkR/Dw+8tKPLtqaNyddPvzpW5TWNuueFbdp2MHDK15aeFK/vXTJY//f1ndFtSZLDIQ3JTolelNLhkL59YbHW7q/X5sqGj3i2TpcOz9a8mZNPue3TQXwAZ4mKI81aW3FMv1q+R/uONOu/rz9X14wrkMPh0Du7j2jG71bLGCnX61FNoPM/xqkJLslIjcHTC5PjUj0ueRPjdbDh9PfExMc5dPPFg1T+oav+omcUZyWr4sjpHSSNfzgn3xsNzL5m3sxJunR4Tkyfk/gAzjKNbSHVN4dUlNn1AmgbDtTLIWl8UbrmLt2jNRVH9ejXx8ub0HkNkpe2HNLB+lZNHJiue57fHr3CbEqCS3c9u1lNHwiU+TdN1sDMJBWkJSo+zqmmYIeW76rTz1/bKafToYe+PEb1LSG9vfuI5q/cp1EFXjW0hJSWFK/2joiq6lv12xsn6oLBWWptD+uqX76lPXXNuueqkdpwoF4vbzms6ecW6IHpo/XU6gPRi+fV+Ns0KDtZSW6Xfv92hW6+eJDyvAnaWNmg9o6whud59dSq/Tra3K5Uj0sPf22shuam6sv/7x01toV02+eHauLAdLnjnLr+f1ZJkkbme7Xj/TeNAZlJmjdzshZuPKjlu+qUmdx5zZuD9a26dESOLh+Ro4uHZavslff0lw1Vykx2Kz3ZrWG5KWoLRVTXGNSlw7OVmeJWTmqCNlc16K1dR7R23zF1RIz6pSUq0BpSY7BD37pgoP531X5NKc7QuMLOj7iO/7/WfmmJ0aAbnJ2sn1w3RhsONGhPXZNe216txrYOeVxO/fOUIpWek6u9R5r1yOs7uxwfNLk4Q+ML03SsuV31LSGdPyhDeb4EPbe+Skt3du4Vcsc5teSuSzT10eXRCxZ+UEayW+P6+/Tm++tnpbj1k+vG6Mm392nl3qNyOKQbJhepLRSWxxWnhHinrhvfT2sqjilijH7++q7o3hyX06GOiFGBL0ELbi7RD/+yRSv3du5JuX5iocb09+l3KypUcaRZ5xWlKSO589iLXTWNOnCsJTqmq8bma0pxhvbUNWtLVYM2vL+nZcKAdLV3RLT1YOdp8WP7+3ReUbq2HfRr75HmE/ZwfHFUnlpC4S57yCYMSFddY7DL9hwO6Q83TVZzMKyDDa1aUV6nt/ccPel1f5LdcXrljosUCht947erVR1o6/K7vGholn48fbS2HQwozunQf/1tuw752+RxdX50ds25Bbr9mY2SpG9dMFCbqxp02+eHaO7SPVq7r/6E7aV6XMrxepTnS9CIPK9+t6JCvsT4k+6ZfPJbk3Tz/65TakK8nvrOFF35i7ckdUbTy7d9jmM+Poj4AHqH9o6I3C6n1lQcU6A1pNL3vyDxk4hEzAn/YesIR7oc0NjQ0q6aQFDD81Llbw3pxU0Hdd15/aMX2PuwcMRoc1WDzu2fdsJz1zd3foQxqsCrcYVpkqQjTUHFO53yJf3jNOWlO2tVkJaooowk7a5t0rDc1OjHFt1hZ3WjBmQmqaq+RbWNQV0wOEttoXCX41U6LyTYoHML07SlqkHr99frkmHZGpqbGl2nvSOi5bvqNL4orcvBkVX1LdpV06iLhmarpT38sadkt75/MPIVo3I1tn+adhwK6PE3y7Xk3Vrde/VI7alt1mF/q2Z/YZgGZHZe6ybJ3XmxQW9CfPT1tLR3aHxR+kdup9rfpiv+e5kGZiXrme+er31HmzU0J1Vul1NHmoJ6ZethDcpK0YVDMuVwOBQKdwZcQVpi9DmOX/dnU2WDEuKdGpH3j/eC2kCb/vvvu3TD5CKN7Z+mSMToxc2HtKXKr5kXDjzhCsTtHRHNXbpHA97/KO/4socWvafmYIe+c1Gx+qUnqiNidKypXf3SExUx5qQHJBtjtP1QQAcbWvXc+iqN6efTNeMKNDArWZIUaAupqa1D+b4Eba7y67Xt1br5okFKT3ZHnyPYEVZ5TZMK0hKV8f7yV7cdlsvpPOHfMX9rSAvWHNBV4wr08KvvKSXBpQeuHd3lwNbKYy3K8yXovP9arMZgR/TrN64cnae535ig3bWNSk2IV643QfPertDa/fX68bWju4wpVogPAECPCbSF5I5znnBQMLrPrppGVdW3qGRQlp5dV6nSkbnq94Ggs4H4AAAAVp3O+3fvugAAAAD4zCM+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMCqbouPJ554QgMHDlRCQoKmTJmiNWvWdNemAABAH9It8fGnP/1Js2fP1n333acNGzZo3Lhxmjp1qmpra7tjcwAAoA9xGGNMrJ90ypQpmjRpkh5//HFJUiQSUWFhoW677TbdfffdXdYNBoMKBoPR+36/X0VFRaqsrJTX64310AAAQDcIBAIqLCxUQ0ODfD7fx67rivXG29vbtX79es2ZMye6zOl0qrS0VCtXrjxh/bKyMt1///0nLC8sLIz10AAAQDdrbGy0Hx9HjhxROBxWbm5ul+W5ubl67733Tlh/zpw5mj17dvR+JBLRsWPHlJmZKYfDEdOxHa8y9qqcGeYxNpjHM8ccxgbzGBtn+zwaY9TY2KiCgoJTrhvz+DhdHo9HHo+ny7K0tLRu3abX6z0r/zBijXmMDebxzDGHscE8xsbZPI+n2uNxXMwPOM3KylJcXJxqamq6LK+pqVFeXl6sNwcAAPqYmMeH2+3WhAkTtGTJkuiySCSiJUuWqKSkJNabAwAAfUy3fOwye/Zs3XjjjZo4caImT56sRx99VM3NzZo5c2Z3bO4T83g8uu+++074mAenh3mMDebxzDGHscE8xgbz+Ml1y6m2kvT444/r4YcfVnV1tc4991w99thjmjJlSndsCgAA9CHdFh8AAAAnw3e7AAAAq4gPAABgFfEBAACsIj4AAIBVZ018PPHEExo4cKASEhI0ZcoUrVmzpqeH1KssX75cV199tQoKCuRwOPT88893edwYo3vvvVf5+flKTExUaWmpysvLu6xz7NgxzZgxQ16vV2lpafr2t7+tpqYmi6+iZ5WVlWnSpElKTU1VTk6Opk+frp07d3ZZp62tTbNmzVJmZqZSUlL0la985YQL8h04cEDTpk1TUlKScnJy9IMf/EAdHR02X0qPmjt3rsaOHRu9SmRJSYkWLVoUfZw5/HQeeughORwO3XnnndFlzOWp/ehHP5LD4ehyGzFiRPRx5vBTMmeBBQsWGLfbbX7/+9+b7du3m+9+97smLS3N1NTU9PTQeo1XXnnF/Od//qf561//aiSZhQsXdnn8oYceMj6fzzz//PNm8+bN5pprrjHFxcWmtbU1us4Xv/hFM27cOLNq1Srz1ltvmSFDhpgbbrjB8ivpOVOnTjVPPvmk2bZtm9m0aZP50pe+ZIqKikxTU1N0ne9///umsLDQLFmyxKxbt86cf/755oILLog+3tHRYUaPHm1KS0vNxo0bzSuvvGKysrLMnDlzeuIl9YgXX3zRvPzyy2bXrl1m586d5j/+4z9MfHy82bZtmzGGOfw01qxZYwYOHGjGjh1r7rjjjuhy5vLU7rvvPjNq1Chz+PDh6K2uri76OHP46ZwV8TF58mQza9as6P1wOGwKCgpMWVlZD46q9/pwfEQiEZOXl2cefvjh6LKGhgbj8XjMM888Y4wxZseOHUaSWbt2bXSdRYsWGYfDYQ4ePGht7L1JbW2tkWSWLVtmjOmcs/j4ePPnP/85us67775rJJmVK1caYzoj0Ol0murq6ug6c+fONV6v1wSDQbsvoBdJT083v/3tb5nDT6GxsdEMHTrULF682FxyySXR+GAuP5n77rvPjBs37qSPMYef3mf+Y5f29natX79epaWl0WVOp1OlpaVauXJlD46s76ioqFB1dXWXOfT5fJoyZUp0DleuXKm0tDRNnDgxuk5paamcTqdWr15tfcy9gd/vlyRlZGRIktavX69QKNRlHkeMGKGioqIu8zhmzJgu3wo9depUBQIBbd++3eLoe4dwOKwFCxaoublZJSUlzOGnMGvWLE2bNq3LnEn8PZ6O8vJyFRQUaNCgQZoxY4YOHDggiTk8Ez3+rbbd7ciRIwqHw11+8ZKUm5ur9957r4dG1bdUV1dL0knn8Phj1dXVysnJ6fK4y+VSRkZGdJ2zSSQS0Z133qkLL7xQo0ePltQ5R263+4Rvbf7wPJ5sno8/drbYunWrSkpK1NbWppSUFC1cuFAjR47Upk2bmMPTsGDBAm3YsEFr16494TH+Hj+ZKVOmaN68eRo+fLgOHz6s+++/XxdddJG2bdvGHJ6Bz3x8AD1h1qxZ2rZtm1asWNHTQ+mThg8frk2bNsnv9+u5557TjTfeqGXLlvX0sPqUyspK3XHHHVq8eLESEhJ6ejh91pVXXhn957Fjx2rKlCkaMGCAnn32WSUmJvbgyPq2z/zHLllZWYqLizvh6OOamhrl5eX10Kj6luPz9HFzmJeXp9ra2i6Pd3R06NixY2fdPN9666166aWX9Oabb6p///7R5Xl5eWpvb1dDQ0OX9T88jyeb5+OPnS3cbreGDBmiCRMmqKysTOPGjdMvfvEL5vA0rF+/XrW1tTrvvPPkcrnkcrm0bNkyPfbYY3K5XMrNzWUuP4W0tDQNGzZMu3fv5u/xDHzm48PtdmvChAlasmRJdFkkEtGSJUtUUlLSgyPrO4qLi5WXl9dlDgOBgFavXh2dw5KSEjU0NGj9+vXRdd544w1FIpGz5gsFjTG69dZbtXDhQr3xxhsqLi7u8viECRMUHx/fZR537typAwcOdJnHrVu3dgm5xYsXy+v1auTIkXZeSC8UiUQUDAaZw9Nw+eWXa+vWrdq0aVP0NnHiRM2YMSP6z8zl6WtqatKePXuUn5/P3+OZ6OkjXm1YsGCB8Xg8Zt68eWbHjh3m5ptvNmlpaV2OPj7bNTY2mo0bN5qNGzcaSeaRRx4xGzduNPv37zfGdJ5qm5aWZl544QWzZcsWc+211570VNvx48eb1atXmxUrVpihQ4eeVafa3nLLLcbn85mlS5d2OS2vpaUlus73v/99U1RUZN544w2zbt06U1JSYkpKSqKPHz8t74orrjCbNm0yr776qsnOzj6rTsu7++67zbJly0xFRYXZsmWLufvuu43D4TCvv/66MYY5PBMfPNvFGObyk7jrrrvM0qVLTUVFhXn77bdNaWmpycrKMrW1tcYY5vDTOiviwxhjfvnLX5qioiLjdrvN5MmTzapVq3p6SL3Km2++aSSdcLvxxhuNMZ2n295zzz0mNzfXeDwec/nll5udO3d2eY6jR4+aG264waSkpBiv12tmzpxpGhsbe+DV9IyTzZ8k8+STT0bXaW1tNf/yL/9i0tPTTVJSkrnuuuvM4cOHuzzPvn37zJVXXmkSExNNVlaWueuuu0woFLL8anrOTTfdZAYMGGDcbrfJzs42l19+eTQ8jGEOz8SH44O5PLXrr7/e5OfnG7fbbfr162euv/56s3v37ujjzOGn4zDGmJ7Z5wIAAM5Gn/ljPgAAQO9CfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYNX/B2J9sgffBZQnAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((torch.tensor(var_cost)*beta+torch.tensor(orth_cost)).detach())\n",
    "\n",
    "plt.ylim(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 20.0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(var_cost).detach())\n",
    "plt.plot(torch.tensor(orth_cost).detach())\n",
    "\n",
    "plt.ylim(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10000x1 and 2x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10000\u001b[39m)[:,\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m----> 2\u001b[0m fx_eval \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_eval\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      3\u001b[0m eigfuncs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(energy\u001b[38;5;241m.\u001b[39mexact_eigfunctions(x_eval\u001b[38;5;241m.\u001b[39mnumpy(), m))\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/eigenfunction-solver/src/eigensolver/neural/network/feedforward.py:93\u001b[0m, in \u001b[0;36mConstantFFN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     92\u001b[0m output[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Assign constant value\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m output[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fill the remaining dimensions\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10000x1 and 2x200)"
     ]
    }
   ],
   "source": [
    "x_eval = torch.linspace(-2,2,10000)[:,None]\n",
    "fx_eval = model(x_eval).detach()\n",
    "eigfuncs = torch.tensor(energy.exact_eigfunctions(x_eval.numpy(), m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconstruct\n",
    "\n",
    "x_pca = torch.tensor(energy.exact_sample((N,)))\n",
    "fx_pca = model(x_pca).detach()[:,1:]\n",
    "\n",
    "cov = torch.cov(fx_pca.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m D, U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigh(cov)\n\u001b[0;32m----> 2\u001b[0m eigvals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[43mbeta\u001b[49m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mD)\n\u001b[1;32m      3\u001b[0m rotation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mD\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;129m@U\u001b[39m\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beta' is not defined"
     ]
    }
   ],
   "source": [
    "D, U = torch.linalg.eigh(cov)\n",
    "eigvals = 2/beta*(1-D)\n",
    "rotation = torch.diag(1/D**(1/2))@U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fx_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fx_eval[:,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[43mfx_eval\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;129m@rotation\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fx_eval' is not defined"
     ]
    }
   ],
   "source": [
    "fx_eval[:,1:] = fx_eval[:,1:]@rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eigvals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(\u001b[43meigvals\u001b[49m)\n\u001b[1;32m      2\u001b[0m fx_eval[:,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m fx_eval[:,\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mindices]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eigvals' is not defined"
     ]
    }
   ],
   "source": [
    "_, indices = torch.sort(eigvals)\n",
    "fx_eval[:,1:] = fx_eval[:,1+indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fx_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Plot array1 with full lines\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(x_eval, \u001b[43mfx_eval\u001b[49m[:, i], color\u001b[38;5;241m=\u001b[39mcolors[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitted m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Plot array2 with dashed lines\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(x_eval, eigfuncs[:, i], color\u001b[38;5;241m=\u001b[39mcolors[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fx_eval' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['black', 'red', 'green', 'blue', 'orange']\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    # Plot array1 with full lines\n",
    "    plt.plot(x_eval, fx_eval[:, i], color=colors[i], label=f'Fitted m = {i}', linestyle='--')\n",
    "    \n",
    "    # Plot array2 with dashed lines\n",
    "    plt.plot(x_eval, eigfuncs[:, i], color=colors[i], label=f'True m = {i}')\n",
    "\n",
    "# Add legend and labels\n",
    "plt.legend()\n",
    "#plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "samples = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rng.choice(samples.detach(), size = num_samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataloader = DataLoader(\n",
    "    x,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lclaeys/miniconda3/envs/efs-env/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
